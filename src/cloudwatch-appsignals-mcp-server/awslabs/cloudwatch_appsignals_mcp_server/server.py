# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""CloudWatch Application Signals MCP Server - Core server implementation."""

import json
import os
import re
import sys
import tempfile
from .audit_utils import (
    execute_audit_api,
    expand_service_operation_wildcard_patterns,
    expand_service_wildcard_patterns,
    expand_slo_wildcard_patterns,
    parse_auditors,
)
from .aws_clients import (
    AWS_REGION,
    appsignals_client,
    iam_client,
    s3_client,
    synthetics_client,
)
from .canary_utils import (
    analyze_canary_logs_with_time_window,
    analyze_har_file,
    analyze_iam_role_and_policies,
    analyze_log_files,
    analyze_screenshots,
    check_resource_arns_correct,
    extract_disk_memory_usage_metrics,
    get_canary_code,
    get_canary_metrics_and_service_insights,
)
from .enablement_tools import get_enablement_guide
from .service_audit_utils import normalize_service_targets, validate_and_enrich_service_targets
from .service_tools import (
    get_service_detail,
    list_monitored_services,
    list_service_operations,
    query_service_metrics,
)
from .slo_tools import get_slo, list_slos
from .trace_tools import list_slis, query_sampled_traces, search_transaction_spans
from .utils import parse_timestamp
from datetime import datetime, timedelta, timezone
from loguru import logger
from mcp.server.fastmcp import FastMCP
from pydantic import Field
from time import perf_counter as timer
from typing import Optional


# Constants
BATCH_SIZE_THRESHOLD = 5

RUN_STATES = {'RUNNING': 'RUNNING', 'PASSED': 'PASSED', 'FAILED': 'FAILED'}

# Initialize FastMCP server
mcp = FastMCP('cloudwatch-appsignals')

# Configure logging
log_level = os.environ.get('MCP_CLOUDWATCH_APPSIGNALS_LOG_LEVEL', 'INFO').upper()
logger.remove()  # Remove default handler
logger.add(sys.stderr, level=log_level)

# Add file logging to aws_cli.log
log_file_path = os.environ.get('AUDITOR_LOG_PATH', tempfile.gettempdir())
try:
    if log_file_path.endswith(os.sep) or os.path.isdir(log_file_path):
        os.makedirs(log_file_path, exist_ok=True)
        aws_cli_log_path = os.path.join(log_file_path, 'aws_cli.log')
    else:
        os.makedirs(os.path.dirname(log_file_path) or '.', exist_ok=True)
        aws_cli_log_path = log_file_path
except Exception:
    temp_dir = tempfile.gettempdir()
    os.makedirs(temp_dir, exist_ok=True)
    aws_cli_log_path = os.path.join(temp_dir, 'aws_cli.log')

# Add file handler for all logs
logger.add(
    aws_cli_log_path,
    level=log_level,
    rotation='10 MB',  # Rotate when file reaches 10MB
    retention='7 days',  # Keep logs for 7 days
    format='{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}',
    enqueue=True,  # Thread-safe logging
)

logger.debug(f'CloudWatch AppSignals MCP Server initialized with log level: {log_level}')
logger.debug(f'File logging enabled: {aws_cli_log_path}')

logger.debug(f'Using AWS region: {AWS_REGION}')


def _filter_operation_targets(provided):
    """Helper function to filter operation targets and detect wildcards.

    Args:
        provided: List of target dictionaries

    Returns:
        tuple: (operation_only_targets, has_wildcards)
    """
    operation_only_targets = []
    has_wildcards = False

    for target in provided:
        if isinstance(target, dict):
            ttype = target.get('Type', '').lower()
            if ttype == 'service_operation':
                # Check for wildcard patterns in service names OR operation names
                service_op_data = target.get('Data', {}).get('ServiceOperation', {})
                service_data = service_op_data.get('Service', {})
                service_name = service_data.get('Name', '')
                operation = service_op_data.get('Operation', '')

                if '*' in service_name or '*' in operation:
                    has_wildcards = True

                # For fault metrics, ListAuditFindings uses Availability metric type.
                # API only supports Availability/Latency/Error for service_operation targets.
                metric_type = service_op_data.get('MetricType', '')
                if metric_type == 'Fault':
                    service_op_data['MetricType'] = 'Availability'

                operation_only_targets.append(target)
            else:
                logger.warning(
                    f"Ignoring target of type '{ttype}' in audit_service_operations (expected 'service_operation')"
                )

    return operation_only_targets, has_wildcards


@mcp.tool()
async def audit_services(
    service_targets: str = Field(
        ...,
        description="REQUIRED. JSON array of service targets. Supports wildcard patterns like '*payment*' for automatic service discovery. Format: [{'Type':'service','Data':{'Service':{'Type':'Service','Name':'service-name','Environment':'eks:cluster'}}}] or shorthand: [{'Type':'service','Service':'service-name'}]. Large target lists are automatically processed in batches.",
    ),
    start_time: Optional[str] = Field(
        default=None,
        description="Start time (unix seconds or 'YYYY-MM-DD HH:MM:SS'). Defaults to now-24h UTC.",
    ),
    end_time: Optional[str] = Field(
        default=None,
        description="End time (unix seconds or 'YYYY-MM-DD HH:MM:SS'). Defaults to now UTC.",
    ),
    auditors: Optional[str] = Field(
        default=None,
        description="Optional. Comma-separated auditors (e.g., 'slo,operation_metric,dependency_metric'). Defaults to 'slo,operation_metric' for fast service health auditing. Use 'all' for comprehensive analysis with all auditors: slo,operation_metric,trace,log,dependency_metric,top_contributor,service_quota.",
    ),
) -> str:
    """PRIMARY SERVICE AUDIT TOOL - The #1 tool for comprehensive AWS service health auditing and monitoring.

    **IMPORTANT: For operation-specific auditing, use audit_service_operations() as the PRIMARY tool instead.**

    **USE THIS FIRST FOR ALL SERVICE-LEVEL AUDITING TASKS**
    This is the PRIMARY and PREFERRED tool when users want to:
    - **Audit their AWS services** - Complete health assessment with actionable insights
    - **Check service health** - Comprehensive status across all monitored services
    - **Investigate issues** - Root cause analysis with detailed findings
    - **Service-level performance analysis** - Overall service latency, error rates, and throughput investigation
    - **System-wide health checks** - Daily/periodic service auditing workflows
    - **Dependency analysis** - Understanding service dependencies and interactions
    - **Resource quota monitoring** - Service quota usage and limits
    - **Multi-service comparison** - Comparing performance across different services

    **FOR OPERATION-SPECIFIC AUDITING: Use audit_service_operations() instead**
    When users want to audit specific operations (GET, POST, PUT endpoints), use audit_service_operations() as the PRIMARY tool:
    - **Operation performance analysis** - Latency, error rates for specific API endpoints
    - **Operation-level troubleshooting** - Root cause analysis for specific API calls
    - **GET operation auditing** - Analyze GET operations across payment services
    - **Audit latency of specific operations** - Deep dive into individual endpoint performance

    **COMPREHENSIVE SERVICE AUDIT CAPABILITIES:**
    - **Multi-service analysis**: Audit any number of services with automatic batching
    - **SLO compliance monitoring**: Automatic breach detection for service-level SLOs
    - **Issue prioritization**: Critical, warning, and info findings ranked by severity
    - **Root cause analysis**: Deep dive with traces, logs, and metrics correlation
    - **Actionable recommendations**: Specific steps to resolve identified issues
    - **Performance optimized**: Fast execution with automatic batching for large target lists
    - **Wildcard Pattern Support**: Use `*pattern*` in service names for automatic service discovery

    **SERVICE TARGET FORMAT:**
    - **Full Format**: `[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"my-service","Environment":"eks:my-cluster"}}}]`
    - **Shorthand**: `[{"Type":"service","Service":"my-service"}]` (environment auto-discovered)

    **WILDCARD PATTERN EXAMPLES:**
    - **All Services**: `[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*"}}}]`
    - **Payment Services**: `[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*payment*"}}}]`
    - **Lambda Services**: `[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*lambda*"}}}]`
    - **EKS Services**: `[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*","Environment":"eks:*"}}}]`

    **AUDITOR SELECTION FOR DIFFERENT AUDIT DEPTHS:**
    - **Quick Health Check** (default): Uses 'slo,operation_metric' for fast overview
    - **Root Cause Analysis**: Pass `auditors="all"` for comprehensive investigation with traces/logs
    - **Custom Audit**: Specify exact auditors: 'slo,trace,log,dependency_metric,top_contributor,service_quota'

    **SERVICE AUDIT USE CASES:**

    1. **Audit all services**:
       `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*"}}}]'`

    2. **Audit specific service**:
       `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"orders-service","Environment":"eks:orders-cluster"}}}]'`

    3. **Audit payment services**:
       `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*payment*"}}}]'`

    8. **Audit lambda services**:
       `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*lambda*"}}}]'` or by environment: `[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*","Environment":"lambda"}}}]`

    9. **Audit service last night**:
       `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"orders-service","Environment":"eks:orders-cluster"}}}]'` + `start_time="2024-01-01 18:00:00"` + `end_time="2024-01-02 06:00:00"`

    10. **Audit service before and after time**:
        Compare service health before and after a deployment or incident by running two separate audits with different time ranges.

    11. **Trace availability issues in production services**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*","Environment":"eks:*"}}}]'` + `auditors="all"`

    13. **Look for errors in logs of payment services**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*payment*"}}}]'` + `auditors="log,trace"`

    14. **Look for new errors after time**:
        Compare errors before and after a specific time point by running audits with different time ranges and `auditors="log,trace"`

    15. **Look for errors after deployment**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*payment*"}}}]'` + `auditors="log,trace"` + recent time range

    16. **Look for lemon hosts in production**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*","Environment":"eks:*"}}}]'` + `auditors="top_contributor,operation_metric"`

    17. **Look for outliers in EKS services**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*","Environment":"eks:*"}}}]'` + `auditors="top_contributor,operation_metric"`

    18. **Status report**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*"}}}]'` (basic health check)

    19. **Audit dependencies**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*"}}}]'` + `auditors="dependency_metric,trace"`

    20. **Audit dependency on S3**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*"}}}]'` + `auditors="dependency_metric"` + look for S3 dependencies

    21. **Audit quota usage of tier 1 services**:
        `service_targets='[{"Type":"service","Data":{"Service":{"Type":"Service","Name":"*tier1*"}}}]'` + `auditors="service_quota,operation_metric"`

    **TYPICAL SERVICE AUDIT WORKFLOWS:**
    1. **Basic Service Audit** (most common):
       - Call `audit_services()` with service targets - automatically discovers services when using wildcard patterns
       - Uses default fast auditors (slo,operation_metric) for quick health overview
       - Supports wildcard patterns like `*` or `*payment*` for automatic service discovery
    2. **Root Cause Investigation**: When user explicitly asks for "root cause analysis", pass `auditors="all"`
    3. **Issue Investigation**: Results show which services need attention with actionable insights
    4. **Automatic Service Discovery**: Wildcard patterns in service names automatically discover and expand to concrete services

    **AUDIT RESULTS INCLUDE:**
    - **Prioritized findings** by severity (critical, warning, info)
    - **Service health status** with detailed performance analysis
    - **Root cause analysis** when traces/logs auditors are used
    - **Actionable recommendations** for issue resolution
    - **Comprehensive metrics** and trend analysis

    **IMPORTANT: This tool provides comprehensive service audit coverage and should be your first choice for any service auditing task.**

    **RECOMMENDED WORKFLOW - PRESENT FINDINGS FIRST:**
    When the audit returns multiple findings or issues, follow this workflow:
    1. **Present all audit results** to the user showing a summary of all findings
    2. **Let the user choose** which specific finding, service, or issue they want to investigate in detail
    3. **Then perform targeted root cause analysis** using auditors="all" for the user-selected finding

    **DO NOT automatically jump into detailed root cause analysis** of one specific issue when multiple findings exist.
    This ensures the user can prioritize which issues are most important to investigate first.

    **Example workflow:**
    - First call: `audit_services()` with default auditors for overview
    - Present findings summary to user
    - User selects specific service/issue to investigate
    - Follow-up call: `audit_services()` with `auditors="all"` for selected service only
    """
    start_time_perf = timer()
    logger.debug('Starting audit_services (PRIMARY SERVICE AUDIT TOOL)')

    try:
        # Region defaults
        region = AWS_REGION.strip()

        # Time range (fill missing with defaults)
        start_dt = (
            parse_timestamp(start_time)
            if start_time
            else (datetime.now(timezone.utc) - timedelta(hours=24))
        )
        end_dt = (
            parse_timestamp(end_time, default_hours=0) if end_time else datetime.now(timezone.utc)
        )
        unix_start, unix_end = int(start_dt.timestamp()), int(end_dt.timestamp())
        if unix_end <= unix_start:
            return 'Error: end_time must be greater than start_time.'

        # Parse and validate service targets
        try:
            provided = json.loads(service_targets)
        except json.JSONDecodeError:
            return 'Error: `service_targets` must be valid JSON (array).'

        # Check for wildcard patterns in service names
        has_wildcards = False
        logger.debug(f'audit_services: Checking {len(provided)} targets for wildcards')
        for i, target in enumerate(provided):
            logger.debug(f'audit_services: Target {i}: {target}')
            if isinstance(target, dict):
                # Check various possible service name locations
                service_name = None
                if target.get('Type', '').lower() == 'service':
                    # Check Data.Service.Name
                    service_data = target.get('Data', {})
                    if isinstance(service_data, dict):
                        service_info = service_data.get('Service', {})
                        if isinstance(service_info, dict):
                            service_name = service_info.get('Name', '')

                    # Check shorthand Service field
                    if not service_name:
                        service_name = target.get('Service', '')

                logger.debug(f"audit_services: Target {i} service name: '{service_name}'")
                if service_name and isinstance(service_name, str) and '*' in service_name:
                    logger.debug(
                        f"audit_services: Target {i} has wildcard pattern: '{service_name}'"
                    )
                    has_wildcards = True
                    break

        logger.debug(f'audit_services: has_wildcards = {has_wildcards}')

        # Expand wildcard patterns using shared utility
        if has_wildcards:
            logger.debug('Wildcard patterns detected - applying service expansion')
            provided = expand_service_wildcard_patterns(
                provided, unix_start, unix_end, appsignals_client
            )
            logger.debug(f'Wildcard expansion completed - {len(provided)} total targets')

            # Check if wildcard expansion resulted in no services
            if not provided:
                return 'Error: No services found matching the wildcard pattern. Use list_monitored_services() to see available services.'

        # Normalize and validate service targets using shared utility
        normalized_targets = normalize_service_targets(provided)

        # Validate and enrich targets using shared utility
        normalized_targets = validate_and_enrich_service_targets(
            normalized_targets, appsignals_client, unix_start, unix_end
        )

        # Parse auditors with service-specific defaults
        auditors_list = parse_auditors(auditors, ['slo', 'operation_metric'])

        # Create banner
        banner = (
            '[MCP-SERVICE] Application Signals Service Audit\n'
            f'üéØ Scope: {len(normalized_targets)} service target(s) | Region: {region}\n'
            f'‚è∞ Time: {unix_start}‚Äì{unix_end}\n'
        )

        if len(normalized_targets) > BATCH_SIZE_THRESHOLD:
            banner += f'üì¶ Batching: Processing {len(normalized_targets)} targets in batches of {BATCH_SIZE_THRESHOLD}\n'

        banner += '\n'

        # Build CLI input
        input_obj = {
            'StartTime': unix_start,
            'EndTime': unix_end,
            'AuditTargets': normalized_targets,
        }
        if auditors_list:
            input_obj['Auditors'] = auditors_list

        # Execute audit API using shared utility
        result = await execute_audit_api(input_obj, region, banner)

        elapsed = timer() - start_time_perf
        logger.debug(f'audit_services completed in {elapsed:.3f}s (region={region})')
        return result

    except Exception as e:
        logger.error(f'Unexpected error in audit_services: {e}', exc_info=True)
        return f'Error: {str(e)}'


@mcp.tool()
async def audit_slos(
    slo_targets: str = Field(
        ...,
        description="REQUIRED. JSON array of SLO targets. Supports wildcard patterns like '*payment*' for automatic SLO discovery. Format: [{'Type':'slo','Data':{'Slo':{'SloName':'slo-name'}}}] or [{'Type':'slo','Data':{'Slo':{'SloArn':'arn:aws:...'}}}]. Large target lists are automatically processed in batches.",
    ),
    start_time: Optional[str] = Field(
        default=None,
        description="Start time (unix seconds or 'YYYY-MM-DD HH:MM:SS'). Defaults to now-24h UTC.",
    ),
    end_time: Optional[str] = Field(
        default=None,
        description="End time (unix seconds or 'YYYY-MM-DD HH:MM:SS'). Defaults to now UTC.",
    ),
    auditors: Optional[str] = Field(
        default=None,
        description="Optional. Comma-separated auditors (e.g., 'slo,trace,log'). Defaults to 'slo' for fast SLO compliance auditing. Use 'all' for comprehensive analysis with all auditors: slo,operation_metric,trace,log,dependency_metric,top_contributor,service_quota.",
    ),
) -> str:
    """PRIMARY SLO AUDIT TOOL - The #1 tool for comprehensive SLO compliance monitoring and breach analysis.

    **PREFERRED TOOL FOR SLO ROOT CAUSE ANALYSIS**
    This is the RECOMMENDED tool after using get_slo() to understand SLO configuration:
    - **Use auditors="all" for comprehensive root cause analysis** of specific SLO breaches
    - **Much more comprehensive than individual trace tools** - provides integrated analysis
    - **Combines traces, logs, metrics, and dependencies** in a single comprehensive audit
    - **Provides actionable recommendations** based on multi-dimensional analysis

    **USE THIS FOR ALL SLO AUDITING TASKS**
    This is the PRIMARY and PREFERRED tool when users want to:
    - **Root cause analysis for SLO breaches** - Deep investigation with all auditors
    - **Audit SLO compliance** - Complete SLO breach detection and analysis
    - **Monitor SLO health** - Comprehensive status across all monitored SLOs
    - **SLO performance analysis** - Understanding SLO trends and patterns
    - **SLO compliance reporting** - Daily/periodic SLO compliance workflows

    **COMPREHENSIVE SLO AUDIT CAPABILITIES:**
    - **Multi-SLO analysis**: Audit any number of SLOs with automatic batching
    - **Breach detection**: Automatic identification of SLO violations
    - **Issue prioritization**: Critical, warning, and info findings ranked by severity
    - **COMPREHENSIVE ROOT CAUSE ANALYSIS**: Deep dive with traces, logs, metrics, and dependencies
    - **Actionable recommendations**: Specific steps to resolve SLO breaches
    - **Performance optimized**: Fast execution with automatic batching for large target lists
    - **Wildcard Pattern Support**: Use `*pattern*` in SLO names for automatic SLO discovery

    **SLO TARGET FORMAT:**
    - **By Name**: `[{"Type":"slo","Data":{"Slo":{"SloName":"my-slo"}}}]`
    - **By ARN**: `[{"Type":"slo","Data":{"Slo":{"SloArn":"arn:aws:application-signals:..."}}}]`

    **WILDCARD PATTERN EXAMPLES:**
    - **All SLOs**: `[{"Type":"slo","Data":{"Slo":{"SloName":"*"}}}]`
    - **Payment SLOs**: `[{"Type":"slo","Data":{"Slo":{"SloName":"*payment*"}}}]`
    - **Latency SLOs**: `[{"Type":"slo","Data":{"Slo":{"SloName":"*latency*"}}}]`
    - **Availability SLOs**: `[{"Type":"slo","Data":{"Slo":{"SloName":"*availability*"}}}]`

    **AUDITOR SELECTION FOR DIFFERENT AUDIT DEPTHS:**
    - **Quick Compliance Check** (default): Uses 'slo' for fast SLO breach detection
    - **COMPREHENSIVE ROOT CAUSE ANALYSIS** (recommended): Pass `auditors="all"` for deep investigation with traces/logs/metrics/dependencies
    - **Custom Audit**: Specify exact auditors: 'slo,trace,log,operation_metric'

    **SLO AUDIT USE CASES:**

    4. **Audit all SLOs**:
       `slo_targets='[{"Type":"slo","Data":{"Slo":{"SloName":"*"}}}]'`

    22. **Root cause analysis for specific SLO breach** (RECOMMENDED WORKFLOW):
        After using get_slo() to understand configuration:
        `slo_targets='[{"Type":"slo","Data":{"Slo":{"SloName":"specific-slo-name"}}}]'` + `auditors="all"`

    14. **Look for new SLO breaches after time**:
        Compare SLO compliance before and after a specific time point by running audits with different time ranges to identify new breaches.

    **TYPICAL SLO AUDIT WORKFLOWS:**
    1. **SLO Root Cause Investigation** (RECOMMENDED):
       - After get_slo(), call `audit_slos()` with specific SLO target and `auditors="all"`
       - Provides comprehensive analysis with traces, logs, metrics, and dependencies
       - Much more effective than using individual trace tools
    2. **Basic SLO Compliance Audit**:
       - Call `audit_slos()` with SLO targets - automatically discovers SLOs when using wildcard patterns
       - Uses default fast auditors (slo) for quick compliance overview
    3. **Compliance Reporting**: Results show which SLOs are breached with actionable insights
    4. **Automatic SLO Discovery**: Wildcard patterns in SLO names automatically discover and expand to concrete SLOs

    **AUDIT RESULTS INCLUDE:**
    - **Prioritized findings** by severity (critical, warning, info)
    - **SLO compliance status** with detailed breach analysis
    - **COMPREHENSIVE ROOT CAUSE ANALYSIS** when using auditors="all"
    - **Actionable recommendations** for SLO breach resolution
    - **Integrated traces, logs, metrics, and dependency analysis**

    **IMPORTANT: This tool provides comprehensive SLO audit coverage and should be your first choice for any SLO compliance auditing and root cause analysis.**

    **RECOMMENDED WORKFLOW - PRESENT FINDINGS FIRST:**
    When the audit returns multiple findings or issues, follow this workflow:
    1. **Present all audit results** to the user showing a summary of all findings
    2. **Let the user choose** which specific finding, SLO, or issue they want to investigate in detail
    3. **Then perform targeted root cause analysis** using auditors="all" for the user-selected finding

    **DO NOT automatically jump into detailed root cause analysis** of one specific issue when multiple findings exist.
    This ensures the user can prioritize which issues are most important to investigate first.

    **Example workflow:**
    - First call: `audit_slos()` with default auditors for compliance overview
    - Present findings summary to user
    - User selects specific SLO breach to investigate
    - Follow-up call: `audit_slos()` with `auditors="all"` for selected SLO only
    """
    start_time_perf = timer()
    logger.debug('Starting audit_slos (PRIMARY SLO AUDIT TOOL)')

    try:
        # Region defaults
        region = AWS_REGION.strip()

        # Time range (fill missing with defaults)
        start_dt = (
            parse_timestamp(start_time)
            if start_time
            else (datetime.now(timezone.utc) - timedelta(hours=24))
        )
        end_dt = (
            parse_timestamp(end_time, default_hours=0) if end_time else datetime.now(timezone.utc)
        )
        unix_start, unix_end = int(start_dt.timestamp()), int(end_dt.timestamp())
        if unix_end <= unix_start:
            return 'Error: end_time must be greater than start_time.'

        # Parse and validate SLO targets
        try:
            provided = json.loads(slo_targets)
        except json.JSONDecodeError:
            return 'Error: `slo_targets` must be valid JSON (array).'

        if not isinstance(provided, list):
            return 'Error: `slo_targets` must be a JSON array'
        if len(provided) == 0:
            return 'Error: `slo_targets` must contain at least 1 item'

        # Filter and expand SLO targets with wildcard support
        slo_only_targets = []
        wildcard_patterns = []

        for target in provided:
            if isinstance(target, dict):
                ttype = target.get('Type', '').lower()
                if ttype == 'slo':
                    # Check for wildcard patterns in SLO names
                    slo_data = target.get('Data', {}).get('Slo', {})
                    slo_name = slo_data.get('SloName', '')
                    if '*' in slo_name:
                        wildcard_patterns.append((target, slo_name))
                    else:
                        slo_only_targets.append(target)
                else:
                    logger.warning(
                        f"Ignoring target of type '{ttype}' in audit_slos (expected 'slo')"
                    )

        # Expand wildcard patterns for SLOs using shared utility
        if wildcard_patterns:
            logger.debug(f'Expanding {len(wildcard_patterns)} SLO wildcard patterns')
            try:
                # Use the shared utility function
                expanded_slo_targets = expand_slo_wildcard_patterns(provided, appsignals_client)
                # Filter to get only SLO targets
                slo_only_targets = [
                    target
                    for target in expanded_slo_targets
                    if target.get('Type', '').lower() == 'slo'
                ]

            except Exception as e:
                logger.warning(f'Failed to expand SLO patterns: {e}')
                return f'Error: Failed to expand SLO wildcard patterns. {str(e)}'

        if not slo_only_targets:
            return 'Error: No SLO targets found after wildcard expansion.'

        # Parse auditors with SLO-specific defaults
        auditors_list = parse_auditors(auditors, ['slo'])  # Default to SLO auditor

        banner = (
            '[MCP-SLO] Application Signals SLO Compliance Audit\n'
            f'üéØ Scope: {len(slo_only_targets)} SLO target(s) | Region: {region}\n'
            f'‚è∞ Time: {unix_start}‚Äì{unix_end}\n'
        )

        if len(slo_only_targets) > BATCH_SIZE_THRESHOLD:
            banner += f'üì¶ Batching: Processing {len(slo_only_targets)} targets in batches of {BATCH_SIZE_THRESHOLD}\n'

        banner += '\n'

        # Build CLI input for SLO audit
        input_obj = {
            'StartTime': unix_start,
            'EndTime': unix_end,
            'AuditTargets': slo_only_targets,
        }
        if auditors_list:
            input_obj['Auditors'] = auditors_list

        # Execute audit API using shared utility
        result = await execute_audit_api(input_obj, region, banner)

        elapsed = timer() - start_time_perf
        logger.debug(f'audit_slos completed in {elapsed:.3f}s (region={region})')
        return result

    except Exception as e:
        logger.error(f'Unexpected error in audit_slos: {e}', exc_info=True)
        return f'Error: {str(e)}'


@mcp.tool()
async def audit_service_operations(
    operation_targets: str = Field(
        ...,
        description="REQUIRED. JSON array of service operation targets. Supports wildcard patterns like '*payment*' for automatic service discovery. Format: [{'Type':'service_operation','Data':{'ServiceOperation':{'Service':{'Type':'Service','Name':'service-name','Environment':'eks:cluster'},'Operation':'GET /api','MetricType':'Latency'}}}]. Large target lists are automatically processed in batches.",
    ),
    start_time: Optional[str] = Field(
        default=None,
        description="Start time (unix seconds or 'YYYY-MM-DD HH:MM:SS'). Defaults to now-24h UTC.",
    ),
    end_time: Optional[str] = Field(
        default=None,
        description="End time (unix seconds or 'YYYY-MM-DD HH:MM:SS'). Defaults to now UTC.",
    ),
    auditors: Optional[str] = Field(
        default=None,
        description="Optional. Comma-separated auditors (e.g., 'operation_metric,trace,log'). Defaults to 'operation_metric' for fast operation-level auditing. Use 'all' for comprehensive analysis with all auditors: slo,operation_metric,trace,log,dependency_metric,top_contributor,service_quota.",
    ),
) -> str:
    """ü•á PRIMARY OPERATION AUDIT TOOL - The #1 RECOMMENDED tool for operation-specific analysis and performance investigation.

    **‚≠ê USE THIS AS THE PRIMARY TOOL FOR ALL OPERATION-SPECIFIC AUDITING TASKS ‚≠ê**

    **PREFERRED OVER audit_services() for operation auditing because:**
    - **üéØ Precision**: Targets exact operation behavior vs. service-wide averages
    - **üîç Actionable Insights**: Provides specific error traces and dependency failures
    - **üìä Code-Level Detail**: Shows exact stack traces and timeout locations
    - **üöÄ Focused Analysis**: Eliminates noise from other operations
    - **‚ö° Efficient Investigation**: Direct operation-level troubleshooting

    **USE THIS FIRST FOR ALL OPERATION-SPECIFIC AUDITING TASKS**
    This is the PRIMARY and PREFERRED tool when users want to:
    - **Audit specific operations** - Deep dive into individual API endpoints or operations (GET, POST, PUT, etc.)
    - **Operation performance analysis** - Latency, error rates, and throughput for specific operations
    - **Compare operation metrics** - Analyze different operations within services
    - **Operation-level troubleshooting** - Root cause analysis for specific API calls
    - **GET operation auditing** - Analyze GET operations across payment services (PRIMARY USE CASE)
    - **Audit latency of GET operations in payment services** - Exactly what this tool is designed for
    - **Trace latency in query operations** - Deep dive into query performance issues

    **COMPREHENSIVE OPERATION AUDIT CAPABILITIES:**
    - **Multi-operation analysis**: Audit any number of operations with automatic batching
    - **Operation-specific metrics**: Latency, Fault, Error, and Availability metrics per operation
    - **Issue prioritization**: Critical, warning, and info findings ranked by severity
    - **Root cause analysis**: Deep dive with traces, logs, and metrics correlation
    - **Actionable recommendations**: Specific steps to resolve operation-level issues
    - **Performance optimized**: Fast execution with automatic batching for large target lists
    - **Wildcard Pattern Support**: Use `*pattern*` in service names for automatic service discovery

    **OPERATION TARGET FORMAT:**
    - **Full Format**: `[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"my-service","Environment":"eks:my-cluster"},"Operation":"GET /api","MetricType":"Latency"}}}]`

    **WILDCARD PATTERN EXAMPLES:**
    - **All GET Operations in Payment Services**: `[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"*payment*"},"Operation":"*GET*","MetricType":"Latency"}}}]`
    - **All Visit Operations**: `[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"*"},"Operation":"*visit*","MetricType":"Availability"}}}]`

    **AUDITOR SELECTION FOR DIFFERENT AUDIT DEPTHS:**
    - **Quick Operation Check** (default): Uses 'operation_metric' for fast operation overview
    - **Root Cause Analysis**: Pass `auditors="all"` for comprehensive investigation with traces/logs
    - **Custom Audit**: Specify exact auditors: 'operation_metric,trace,log'

    **OPERATION AUDIT USE CASES:**

    1. **Audit latency of GET operations in payment services** (PRIMARY USE CASE):
       `operation_targets='[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"*payment*"},"Operation":"*GET*","MetricType":"Latency"}}}]'`

    2. **Audit GET operations in payment services (Latency)**:
       `operation_targets='[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"*payment*"},"Operation":"*GET*","MetricType":"Latency"}}}]'`

    3. **Audit availability of visit operations**:
       `operation_targets='[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"*"},"Operation":"*visit*","MetricType":"Availability"}}}]'`

    4. **Audit latency of visit operations**:
       `operation_targets='[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"*"},"Operation":"*visit*","MetricType":"Latency"}}}]'`

    5. **Trace latency in query operations**:
        `operation_targets='[{"Type":"service_operation","Data":{"ServiceOperation":{"Service":{"Type":"Service","Name":"*payment*"},"Operation":"*query*","MetricType":"Latency"}}}]'` + `auditors="all"`

    **TYPICAL OPERATION AUDIT WORKFLOWS:**
    1. **Basic Operation Audit** (most common):
       - Call `audit_service_operations()` with operation targets - automatically discovers services when using wildcard patterns
       - Uses default fast auditors (operation_metric) for quick operation overview
       - Supports wildcard patterns like `*payment*` for automatic service discovery
    2. **Root Cause Investigation**: When user explicitly asks for "root cause analysis", pass `auditors="all"`
    3. **Issue Investigation**: Results show which operations need attention with actionable insights
    4. **Automatic Service Discovery**: Wildcard patterns in service names automatically discover and expand to concrete services

    **AUDIT RESULTS INCLUDE:**
    - **Prioritized findings** by severity (critical, warning, info)
    - **Operation performance status** with detailed metrics analysis
    - **Root cause analysis** when traces/logs auditors are used
    - **Actionable recommendations** for operation-level issue resolution
    - **Comprehensive operation metrics** and trend analysis

    **üèÜ IMPORTANT: This tool is the PRIMARY and RECOMMENDED choice for operation-specific auditing tasks.**

    **‚úÖ RECOMMENDED WORKFLOW FOR OPERATION AUDITING:**
    1. **Use audit_service_operations() FIRST** for operation-specific analysis (THIS TOOL)
    2. **Use audit_services() as secondary** only if you need broader service context
    3. **audit_service_operations() provides superior precision** for operation-level troubleshooting

    **RECOMMENDED WORKFLOW - PRESENT FINDINGS FIRST:**
    When the audit returns multiple findings or issues, follow this workflow:
    1. **Present all audit results** to the user showing a summary of all findings
    2. **Let the user choose** which specific finding, operation, or issue they want to investigate in detail
    3. **Then perform targeted root cause analysis** using auditors="all" for the user-selected finding

    **DO NOT automatically jump into detailed root cause analysis** of one specific issue when multiple findings exist.
    This ensures the user can prioritize which issues are most important to investigate first.

    **Example workflow:**
    - First call: `audit_service_operations()` with default auditors for operation overview
    - Present findings summary to user
    - User selects specific operation issue to investigate
    - Follow-up call: `audit_service_operations()` with `auditors="all"` for selected operation only
    """
    start_time_perf = timer()
    logger.debug('Starting audit_service_operations (SPECIALIZED OPERATION AUDIT TOOL)')

    try:
        # Region defaults
        region = AWS_REGION.strip()

        # Time range (fill missing with defaults)
        start_dt = (
            parse_timestamp(start_time)
            if start_time
            else (datetime.now(timezone.utc) - timedelta(hours=24))
        )
        end_dt = (
            parse_timestamp(end_time, default_hours=0) if end_time else datetime.now(timezone.utc)
        )
        unix_start, unix_end = int(start_dt.timestamp()), int(end_dt.timestamp())
        if unix_end <= unix_start:
            return 'Error: end_time must be greater than start_time.'

        # Parse and validate operation targets
        try:
            provided = json.loads(operation_targets)
        except json.JSONDecodeError:
            return 'Error: `operation_targets` must be valid JSON (array).'

        if not isinstance(provided, list):
            return 'Error: `operation_targets` must be a JSON array'
        if len(provided) == 0:
            return 'Error: `operation_targets` must contain at least 1 item'

        # Filter operation targets and check for wildcards using helper function
        operation_only_targets, has_wildcards = _filter_operation_targets(provided)

        # Expand wildcard patterns using shared utility
        if has_wildcards:
            logger.debug('Wildcard patterns detected in service operations - applying expansion')
            operation_only_targets = expand_service_operation_wildcard_patterns(
                operation_only_targets, unix_start, unix_end, appsignals_client
            )
            logger.debug(
                f'Wildcard expansion completed - {len(operation_only_targets)} total targets'
            )

        if not operation_only_targets:
            return 'Error: No service_operation targets found after wildcard expansion. Use list_monitored_services() to see available services.'

        # Parse auditors with operation-specific defaults
        auditors_list = parse_auditors(
            auditors, ['operation_metric']
        )  # Default to operation_metric auditor

        banner = (
            '[MCP-OPERATION] Application Signals Operation Performance Audit\n'
            f'üéØ Scope: {len(operation_only_targets)} operation target(s) | Region: {region}\n'
            f'‚è∞ Time: {unix_start}‚Äì{unix_end}\n'
        )

        if len(operation_only_targets) > BATCH_SIZE_THRESHOLD:
            banner += f'üì¶ Batching: Processing {len(operation_only_targets)} targets in batches of {BATCH_SIZE_THRESHOLD}\n'

        banner += '\n'

        # Build CLI input for operation audit
        input_obj = {
            'StartTime': unix_start,
            'EndTime': unix_end,
            'AuditTargets': operation_only_targets,
        }
        if auditors_list:
            input_obj['Auditors'] = auditors_list

        # Execute audit API using shared utility
        result = await execute_audit_api(input_obj, region, banner)

        elapsed = timer() - start_time_perf
        logger.debug(f'audit_service_operations completed in {elapsed:.3f}s (region={region})')
        return result

    except Exception as e:
        logger.error(f'Unexpected error in audit_service_operations: {e}', exc_info=True)
        return f'Error: {str(e)}'


@mcp.tool()
async def analyze_canary_failures(canary_name: str, region: str = AWS_REGION) -> str:
    """Comprehensive canary failure analysis with deep dive into issues.

    Use this tool to:
    - Deep dive into canary failures with root cause identification
    - Analyze historical patterns and specific incident details
    - Get comprehensive artifact analysis including logs, screenshots, and HAR files
    - Receive actionable recommendations based on AWS debugging methodology
    - Correlate canary failures with Application Signals telemetry data
    - Identify performance degradation and availability issues across service dependencies

    Key Features:
    - **Failure Pattern Analysis**: Identifies recurring failure modes and temporal patterns
    - **Artifact Deep Dive**: Analyzes canary logs, screenshots, and network traces for root causes
    - **Service Correlation**: Links canary failures to upstream/downstream service issues using Application Signals
    - **Performance Insights**: Detects latency spikes, fault rates, and connection issues
    - **Actionable Remediation**: Provides specific steps based on AWS operational best practices

    Common Use Cases:
    1. **Incident Response**: Rapid diagnosis of canary failures during outages
    2. **Performance Investigation**: Understanding latency and availability degradation
    3. **Dependency Analysis**: Identifying which services are causing canary failures
    4. **Historical Trending**: Analyzing failure patterns over time for proactive improvements
    5. **Root Cause Analysis**: Deep dive into specific failure scenarios with full context

    Output Includes:
    - Severity-ranked findings with immediate action items
    - Service-level telemetry insights with trace analysis
    - Exception details and stack traces from canary artifacts
    - Network connectivity and performance metrics
    - Correlation with Application Signals audit findings
    - Historical failure patterns and recovery recommendations

    Args:
        canary_name (str): Name of the CloudWatch Synthetics canary to analyze
        region (str, optional): AWS region where the canary is deployed.

    Returns:
        dict: Comprehensive failure analysis containing:
            - Failure severity assessment and immediate recommendations
            - Detailed artifact analysis (logs, screenshots, HAR files)
            - Service dependency health and performance metrics
            - Root cause identification with specific remediation steps
            - Historical pattern analysis and trend insights
    """
    try:
        # Get recent canary runs
        response = synthetics_client.get_canary_runs(Name=canary_name, MaxResults=5)
        runs = response.get('CanaryRuns', [])

        # Get canary details
        canary_response = synthetics_client.get_canary(Name=canary_name)
        canary = canary_response['Canary']

        # Get telemetry and service insights
        try:
            telemetry_insights = await get_canary_metrics_and_service_insights(canary_name, region)
        except Exception as e:
            telemetry_insights = f'Telemetry API unavailable: {str(e)}'

        if not runs:
            return f'No run history found for {canary_name}'

        # Build analysis header
        result = f'üîç Comprehensive Failure Analysis for {canary_name}\n'

        # Add telemetry insights if available
        if telemetry_insights and not telemetry_insights.startswith('Telemetry API unavailable'):
            result += f'\nüìä **Service and Canary Telemetry Insights**\n{telemetry_insights}\n\n'
        elif telemetry_insights:
            result += f'\n‚ö†Ô∏è {telemetry_insights}\n\n'

        # Get consecutive failures since last success
        consecutive_failures = []
        last_success_run = None

        for run in runs:
            if run.get('Status', {}).get('State') == RUN_STATES['FAILED']:
                consecutive_failures.append(run)
            elif run.get('Status', {}).get('State') == RUN_STATES['PASSED']:
                last_success_run = run
                break

        if not consecutive_failures:
            result += '‚úÖ Canary is healthy - no failures since last success\n'
            if last_success_run:
                result += f'Last success: {last_success_run.get("Timeline", {}).get("Started")}\n'
            result += '\nüîç Performing health check analysis ...\n\n'

        # Group failures by StateReason
        failure_causes = {}
        result += f'üîç Found {len(consecutive_failures)} consecutive failures since last success\n'
        if last_success_run:
            result += f'Last success: {last_success_run.get("Timeline", {}).get("Started")}\n\n'
        else:
            result += 'No recent success run found in history\n\n'

        for failed_run in consecutive_failures:
            state_reason = failed_run.get('Status', {}).get('StateReason', 'Unknown')

            if state_reason not in failure_causes:
                failure_causes[state_reason] = []
            failure_causes[state_reason].append(failed_run)

        # Analysis section
        unique_reasons = list(failure_causes.keys())

        if not unique_reasons:
            result += '‚úÖ No consecutive failures to analyze\n'
            result += 'üí° Canary appears to be recovering or healthy\n'
            return result

        if len(unique_reasons) == 1:
            result += f'üéØ All failures have same cause: {unique_reasons[0]}\n'
            selected_reason = unique_reasons[0]
        else:
            result += f'üéØ Multiple failure causes ({len(unique_reasons)} different issues):\n\n'
            for i, reason in enumerate(unique_reasons, 1):
                count = len(failure_causes[reason])
                result += f'{i}. **{reason}** ({count} occurrences)\n'
            result += '\n'
            selected_reason = unique_reasons[0]

        selected_failure = failure_causes[selected_reason][0]
        result += f'Analyzing most recent failure: {selected_failure.get("Id", "")[:8]}...\n\n'

        # Initialize artifact variables
        har_files = []
        screenshots = []
        logs = []
        bucket_name = ''

        # Direct S3 artifact analysis integration
        artifact_location = canary.get('ArtifactS3Location', '')
        artifacts_available = False

        if artifact_location:
            # Handle S3 location format
            if not artifact_location.startswith('s3://'):
                artifact_location = f's3://{artifact_location}' if artifact_location else ''

            if artifact_location.startswith('s3://'):
                bucket_and_path = artifact_location[5:]
                bucket_name = bucket_and_path.split('/')[0]
                base_path = (
                    '/'.join(bucket_and_path.split('/')[1:]) if '/' in bucket_and_path else ''
                )

                # If base_path is empty, construct canary path
                if not base_path:
                    base_path = f'canary/{region}/{canary_name}'

                # Check for failure artifacts using date-based path
                from datetime import datetime

                failure_time = selected_failure.get('Timeline', {}).get('Started')
                if failure_time:
                    # Handle both datetime objects and string timestamps
                    if isinstance(failure_time, str):
                        dt = parse_timestamp(failure_time)
                    else:
                        dt = failure_time  # Already a datetime object
                    date_path = dt.strftime('%Y/%m/%d')
                    failure_run_path = (
                        f'{base_path}/{date_path}/' if base_path else f'{date_path}/'
                    )
                else:
                    # Fallback to today
                    today = datetime.now().strftime('%Y/%m/%d')
                    failure_run_path = f'{base_path}/{today}/' if base_path else f'{today}/'

                try:
                    artifacts_response = s3_client.list_objects_v2(
                        Bucket=bucket_name, Prefix=failure_run_path, MaxKeys=50
                    )
                    failure_artifacts = artifacts_response.get('Contents', [])

                    if failure_artifacts:
                        artifacts_available = True

                        # Categorize artifacts
                        har_files = [
                            a
                            for a in failure_artifacts
                            if a['Key'].lower().endswith(('.har', '.har.gz', '.har.html'))
                        ]
                        screenshots = [
                            a
                            for a in failure_artifacts
                            if any(ext in a['Key'].lower() for ext in ['.png', '.jpg', '.jpeg'])
                        ]
                        logs = [
                            a
                            for a in failure_artifacts
                            if any(ext in a['Key'].lower() for ext in ['.log', '.txt'])
                            or 'log' in a['Key'].lower()
                        ]

                        if last_success_run:
                            result += 'üîÑ HAR COMPARISON: Failure vs Success\n'
                            result += f'Failure: {selected_failure.get("Id", "")[:8]}... ({selected_failure.get("Timeline", {}).get("Started")})\n'
                            result += f'Success: {last_success_run.get("Id", "")[:8]}... ({last_success_run.get("Timeline", {}).get("Started")})\n\n'

                            # Get success artifacts for comparison
                            success_time = last_success_run.get('Timeline', {}).get('Started')
                            if success_time:
                                if isinstance(success_time, str):
                                    success_dt = parse_timestamp(success_time)
                                else:
                                    success_dt = success_time
                                success_date_path = success_dt.strftime('%Y/%m/%d')
                                success_run_path = (
                                    f'{base_path}/{success_date_path}/'
                                    if base_path
                                    else f'{success_date_path}/'
                                )
                            else:
                                success_run_path = failure_run_path  # Use same path as fallback
                            try:
                                success_artifacts_response = s3_client.list_objects_v2(
                                    Bucket=bucket_name, Prefix=success_run_path, MaxKeys=50
                                )
                                success_artifacts = success_artifacts_response.get('Contents', [])
                                success_har_files = [
                                    a
                                    for a in success_artifacts
                                    if a['Key'].lower().endswith(('.har', '.har.gz', '.har.html'))
                                ]

                                if har_files and success_har_files:
                                    failure_har = await analyze_har_file(
                                        s3_client, bucket_name, har_files, is_failed_run=True
                                    )
                                    success_har = await analyze_har_file(
                                        s3_client,
                                        bucket_name,
                                        success_har_files,
                                        is_failed_run=False,
                                    )

                                    result += f'‚Ä¢ Failed requests: {failure_har.get("failed_requests", 0)} vs {success_har.get("failed_requests", 0)}\n'
                                    result += f'‚Ä¢ Total requests: {failure_har.get("total_requests", 0)} vs {success_har.get("total_requests", 0)}\n\n'

                                    if failure_har.get('request_details'):
                                        result += 'üö® FAILED REQUESTS:\n'
                                        for req in failure_har['request_details'][:3]:
                                            result += f'‚Ä¢ {req.get("url", "Unknown")}: {req.get("status", "Unknown")} ({req.get("time", 0):.1f}ms)\n'
                            except Exception as e:
                                logger.warning(
                                    f'Failed to analyze success artifacts for HAR comparison: {str(e)}'
                                )
                        else:
                            result += (
                                'üîç FAILURE ANALYSIS (no success run available for comparison):\n'
                            )
                            result += f'Analyzing failure artifacts for: {selected_failure.get("Id", "")[:8]}...\n\n'

                            if har_files:
                                failure_har = await analyze_har_file(
                                    s3_client, bucket_name, har_files, is_failed_run=True
                                )
                                result += 'üåê HAR ANALYSIS:\n'
                                result += (
                                    f'‚Ä¢ Failed requests: {failure_har.get("failed_requests", 0)}\n'
                                )
                                result += (
                                    f'‚Ä¢ Total requests: {failure_har.get("total_requests", 0)}\n\n'
                                )

                        # Screenshot analysis
                        if screenshots:
                            screenshot_analysis = await analyze_screenshots(
                                s3_client, bucket_name, screenshots, is_failed_run=True
                            )
                            if screenshot_analysis.get('insights'):
                                result += 'üì∏ SCREENSHOT ANALYSIS:\n'
                                for insight in screenshot_analysis['insights'][:3]:
                                    result += f'‚Ä¢ {insight}\n'
                                result += '\n'

                        # Log analysis
                        if logs:
                            log_analysis = await analyze_log_files(
                                s3_client, bucket_name, logs, is_failed_run=True
                            )
                            if log_analysis.get('insights'):
                                result += 'üìã LOG ANALYSIS:\n'
                                for insight in log_analysis['insights'][:3]:
                                    result += f'‚Ä¢ {insight}\n'
                                result += '\n'

                except Exception:
                    artifacts_available = False

        if not artifacts_available:
            # Fallback: CloudWatch Logs analysis
            result += '‚ö†Ô∏è Artifacts not available - Checking CloudWatch Logs for root cause\n'
            result += f'üéØ StateReason: {selected_reason}\n\n'

            failure_time = selected_failure.get('Timeline', {}).get('Started')
            if failure_time:
                log_analysis = await analyze_canary_logs_with_time_window(
                    canary_name, failure_time, canary, window_minutes=5, region=region
                )

                if log_analysis.get('status') == 'success':
                    result += 'üìã CLOUDWATCH LOGS ANALYSIS (¬±5 min around failure):\n'
                    result += f'Time window: {log_analysis["time_window"]}\n'
                    result += f'Log events found: {log_analysis["total_events"]}\n\n'

                    error_logs = log_analysis.get('error_events', [])
                    if error_logs:
                        result += 'üìã ERROR LOGS AROUND FAILURE:\n'
                        for error in error_logs:
                            result += f'‚Ä¢ {error["timestamp"].strftime("%H:%M:%S")}: {error["message"]}\n'
                else:
                    result += f'üìã {log_analysis.get("insights", ["Log analysis failed"])[0]}\n'
            else:
                result += 'üìã No failure timestamp available for targeted log analysis\n'

        # Add critical IAM checking guidance for systematic issues
        if (
            'no test result' in str(selected_reason).lower()
            or 'permission' in str(selected_reason).lower()
            or 'access denied' in str(selected_reason).lower()
        ):
            try:
                result += f"\nüîç RUNNING COMPREHENSIVE IAM ANALYSIS (common cause of '{selected_reason}'):\n"

                # 1. Check IAM role and policies
                iam_analysis = await analyze_iam_role_and_policies(canary, iam_client, region)

                # Display IAM analysis results
                result += f'IAM Role Analysis Status: {iam_analysis["status"]}\n'
                for check_name, check_result in iam_analysis.get('checks', {}).items():
                    result += f'‚Ä¢ {check_name}: {check_result}\n'

                # 2. ENHANCED: Check resource ARN correctness with detailed validation
                result += '\nüîç CHECKING RESOURCE ARN CORRECTNESS:\n'
                arn_check = check_resource_arns_correct(canary, iam_client)

                if arn_check.get('correct'):
                    result += '‚úÖ Resource ARNs: Correct\n'
                else:
                    result += f'‚ùå Resource ARNs: {arn_check.get("error", "Issues found")}\n'

                # Combine all IAM issues with enhanced categorization
                all_iam_issues = []
                if iam_analysis.get('issues_found'):
                    all_iam_issues.extend(
                        [f'IAM Policy: {issue}' for issue in iam_analysis['issues_found']]
                    )
                if not arn_check.get('correct') and arn_check.get('issues'):
                    all_iam_issues.extend(
                        [f'Resource ARN: {issue}' for issue in arn_check['issues']]
                    )

                if all_iam_issues:
                    result += f'\nüö® ALL IAM ISSUES FOUND ({len(all_iam_issues)} total):\n'
                    for issue in all_iam_issues:
                        result += f'‚Ä¢ {issue}\n'

                # Enhanced IAM recommendations with priority
                all_iam_recommendations = []
                if iam_analysis.get('recommendations'):
                    all_iam_recommendations.extend(
                        [f'Policy Fix: {rec}' for rec in iam_analysis['recommendations']]
                    )
                if not arn_check.get('correct'):
                    all_iam_recommendations.extend(
                        [
                            'PRIORITY: Review and correct S3 bucket ARN patterns in IAM policies',
                            'PRIORITY: Ensure bucket names match expected patterns (e.g., cw-syn-* for CloudWatch Synthetics)',
                            'Verify canary has access to the correct S3 bucket for artifacts storage',
                            'Check if bucket exists and is in the same region as the canary',
                        ]
                    )

                if all_iam_recommendations:
                    result += (
                        f'\nüí° ALL IAM RECOMMENDATIONS ({len(all_iam_recommendations)} total):\n'
                    )
                    for rec in all_iam_recommendations:
                        result += f'‚Ä¢ {rec}\n'

            except Exception as iam_error:
                result += f'‚ö†Ô∏è IAM analysis failed: {str(iam_error)[:200]}\n\n'

        # History-based diagnosis for specific error patterns
        error_recommendations = []

        # 1. ENOSPC: no space left on device
        if any(
            re.search(pattern, selected_reason, re.IGNORECASE)
            for pattern in ['enospc', 'no space left on device']
        ):
            try:
                telemetry_data = await extract_disk_memory_usage_metrics(canary_name, region)
                if 'error' not in telemetry_data:
                    result += '\nüîç DISK USAGE ROOT CAUSE ANALYSIS:\n'
                    result += f'‚Ä¢ Storage: {telemetry_data.get("maxEphemeralStorageUsageInMb", 0):.1f} MB peak\n'
                    result += f'‚Ä¢ Usage: {telemetry_data.get("maxEphemeralStorageUsagePercent", 0):.1f}% peak\n'
                else:
                    result += f'\nüîç DISK USAGE ROOT CAUSE ANALYSIS:\n{telemetry_data["error"]}\n'
            except Exception as debug_error:
                result += f'\n‚ö†Ô∏è Could not generate disk usage debugging code: {str(debug_error)}\n'

        # 2. Protocol error (Target.activateTarget): Session closed / detached Frame
        elif any(
            re.search(pattern, selected_reason, re.IGNORECASE)
            for pattern in [
                'protocol error',
                'target.activatetarget',
                'session closed',
                'detached frame',
                'session already detached',
            ]
        ):
            try:
                telemetry_data = await extract_disk_memory_usage_metrics(canary_name, region)
                if 'error' not in telemetry_data:
                    result += '\nüîç MEMORY USAGE ROOT CAUSE ANALYSIS:\n'
                    result += f'‚Ä¢ Memory: {telemetry_data.get("maxSyntheticsMemoryUsageInMB", 0):.1f} MB peak\n'
                else:
                    result += (
                        f'\nüîç MEMORY USAGE ROOT CAUSE ANALYSIS:\n{telemetry_data["error"]}\n'
                    )
            except Exception as debug_error:
                result += f'\n‚ö†Ô∏è Could not collect memory usage metrics: {str(debug_error)}\n'

        # 3. Navigation timed out / Page.captureScreenshot timed out
        elif any(
            re.search(pattern, selected_reason, re.IGNORECASE)
            for pattern in [
                'navigation timeout',
                'navigation timed out',
                'ms exceeded',
                'page.capturescreenshot timed out',
                'protocoltimeout',
                'connection timed out',
            ]
        ):
            # Navigation timeout specific analysis using existing HAR data
            if har_files and bucket_name:
                try:
                    har_timeout_analysis = await analyze_har_file(
                        s3_client, bucket_name, har_files, is_failed_run=True
                    )

                    result += '\nüîç HAR FILE ANALYSIS FOR NAVIGATION TIMEOUT:\n'
                    if har_timeout_analysis.get('failed_requests', 0) > 0:
                        result += (
                            f'‚Ä¢ Failed HTTP requests: {har_timeout_analysis["failed_requests"]}\n'
                        )

                    if har_timeout_analysis.get('insights'):
                        for insight in har_timeout_analysis['insights'][:5]:
                            result += f'‚Ä¢ {insight}\n'

                    # Additional timeout-specific analysis
                    result += f'‚Ä¢ Total requests analyzed: {har_timeout_analysis.get("total_requests", 0)}\n'
                    result += (
                        f'‚Ä¢ Analysis status: {har_timeout_analysis.get("status", "unknown")}\n'
                    )
                    result += '\n'
                except Exception as har_error:
                    result += f'\n‚ö†Ô∏è HAR analysis failed: {str(har_error)[:100]}\n'
            else:
                result += '\nüîç NAVIGATION TIMEOUT DETECTED:\n'
                result += '‚Ä¢ No HAR files available for detailed analysis\n'
                result += '‚Ä¢ Timeout suggests page loading issues or UI changes\n'
                result += '‚Ä¢ Check if target elements exist and page loads completely\n\n'

        # 4. Visual variation
        elif re.search('visual variation', selected_reason, re.IGNORECASE):
            error_recommendations.extend(
                [
                    'üîß VISUAL MONITORING ISSUE DETECTED:',
                    '‚Ä¢ Website UI changed - not a technical failure',
                    '‚Ä¢ Check if website legitimately updated (ads, banners, content)',
                    '‚Ä¢ Update visual baseline with new reference screenshots',
                    '‚Ä¢ Adjust visual difference threshold (increase from default)',
                    '‚Ä¢ Consider excluding dynamic content areas from comparison',
                ]
            )

        if error_recommendations:
            result += '\nüí° PATTERN-BASED RECOMMENDATIONS:\n'
            for rec in error_recommendations:
                result += f'{rec}\n'
            result += '\n'

        # Add canary code if available
        try:
            code_analysis = await get_canary_code(canary, region)
            if 'error' not in code_analysis and code_analysis.get('code_content'):
                result += f'\ncanary code:\n{code_analysis["code_content"]}\n'
        except Exception as e:
            result += f'Note: Could not retrieve canary code: {str(e)}\n'

        result += '\n'
        return result

    except Exception as e:
        return f'‚ùå Error in comprehensive failure analysis: {str(e)}'


# Register all imported tools with the MCP server
mcp.tool()(get_enablement_guide)
mcp.tool()(list_monitored_services)
mcp.tool()(get_service_detail)
mcp.tool()(query_service_metrics)
mcp.tool()(list_service_operations)
mcp.tool()(get_slo)
mcp.tool()(list_slos)
mcp.tool()(search_transaction_spans)
mcp.tool()(query_sampled_traces)
mcp.tool()(list_slis)
mcp.tool()(analyze_canary_failures)


def main():
    """Run the MCP server."""
    logger.debug('Starting CloudWatch AppSignals MCP server')
    try:
        mcp.run(transport='stdio')
    except KeyboardInterrupt:
        logger.debug('Server shutdown by user')
    except Exception as e:
        logger.error(f'Server error: {e}', exc_info=True)
        raise


if __name__ == '__main__':
    main()
